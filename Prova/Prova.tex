\documentclass[final,3p]{elsarticle}

% \documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}
\usepackage[portuguese]{babel}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{tablefootnote}

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepgfplotslibrary{statistics}
\usepackage{pgfplotstable}

\usepackage{placeins}
\usepackage{hyperref}
\numberwithin{equation}{section}

\usepackage{algorithm}
\usepackage[noEnd=true, indLines=true]{algpseudocodex}
\algrenewcommand\algorithmicrequire{\textbf{Entrada:}}
\algrenewcommand\algorithmicwhile{\textbf{Enquanto}}
\algrenewcommand\algorithmicrepeat{\textbf{Repete}}
\algrenewcommand\algorithmicuntil{\textbf{Até}}
\algrenewcommand\algorithmicif{\textbf{Se}}
\algrenewcommand\algorithmicthen{\textbf{então}}
\algrenewcommand\algorithmicelse{\textbf{Caso contrário}}
\algrenewcommand\algorithmicensure{\textbf{Objetivo:}}
\algrenewcommand\algorithmicreturn{\textbf{Retorna:}}
\algrenewcommand\algorithmicdo{\textbf{faça}}
\algrenewcommand\algorithmicforall{\textbf{Para todos}}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) \textcolor{black!50}{\emph{#1}}}

\newcommand*{\squareb}{\textcolor{black}{\rule{0.5em}{0.5em}}}
\newcommand*{\squareg}{\textcolor{gray}{\rule{0.5em}{0.5em}}}

\graphicspath{ {./png/} }

% \usepackage[fleqn]{nccmath}
% \usepackage{multicol}


%=========== Gloabal Tikz settings
% \pgfplotsset{compat=newest}
% \usetikzlibrary{math}
% \pgfplotsset{
%     height = 10cm,
%     width = 10cm,
%     tick pos = left,
%     legend style={at={(0.98,0.30)}, anchor=east},
%     legend cell align=left,
%     }
%  \pgfkeys{
%     /pgf/number format/.cd,
%     fixed,
%     precision = 1,
%     set thousands separator = {}
% }

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.98,0.98,0.98}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% \journal{Nuclear Physics B}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{IA048 - Aprendizado de Máquina: Prova 28/05/2024}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{Tiago Corrêa de Araújo de Amorim (RA: 100675)}
% \affiliation[label1]{organization={Doutorando no Departamento de Engenharia de Petróleo da Faculdade de Engenharia Mecânica, UNICAMP},
            % city={Campinas},
            % state={SP},
            % country={Brasil}}

% \author[label2]{\\Taylon Luan Congio Martins (RA: 177379) \texttt{t177379@m.unicamp.br}}
% \affiliation[label2]{organization={Aluno especial, UNICAMP},
%             city={Campinas},
%             state={SP},
%             country={Brasil}}

% \begin{abstract}
%     xxxxxxx
% \end{abstract}

%%Graphical abstract
% \begin{graphicalabstract}
%\includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

% \begin{keyword}
%     Fluxo em Meio Poroso \sep Redes Neurais Recorrentes \sep Rede Neurais Convolucionais
% %% keywords here, in the form: keyword \sep keyword

% %% PACS codes here, in the form: \PACS code \sep code

% %% MSC codes here, in the form: \MSC code \sep code
% %% or \MSC[2008] code \sep code (2000 is the default)

% \end{keyword}

\end{frontmatter}

%% main text
\section{Questão 1}

    As redes neurais generativas adversárias (GANs, generative adversarial networks) trouxeram uma abordagem inovadora para a área de aprendizado de máquina.

    \begin{enumerate}[(a)]
        \item (0,8) Explique por que as redes geradora e discriminadora são consideradas adversárias.
        \item (0,8) Tendo em vista os papeis desempenhados por cada rede, discorra sobre a função custo proposta por Goodfellow et al. (2014)\footnote{Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., “Generative Adversarial Networks”, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), pp.2672-2680, 2014.} para o treinamento da GAN. Complementando essa análise, comente também sobre como a rede geradora consegue aprender as características típicas dos dados reais.
    \end{enumerate}

    \subsection{Resposta \textbf{(a)}}

        As redes geradora e discriminadora são consideradas adversárias porque o treinamento dos seus pesos tem objetivos conflitantes. A função objetivo da rede discriminadora é maximizar o número de vezes em que classifica os dados reais como verdadeiros e os dados gerados pela rede geradora como falsos. Já a rede geradora tem como função objetivo maximizar o número de vezes em que a rede discriminadora classifica a saída da rede geradora como verdadeiros. É este jogo de gato e rato que faz com que a rede geradora vá aprendendo a gerar amostras cada vez mais parecidas com as amostras reais, mesmo sem explicitamente definir o que é uma amostra parecida com uma amostra real.

    \subsection{Resposta \textbf{(b)}}

        A função custo proposta por Goodfellow segue o que foi discutido no item anterior. Os pesos da rede discriminadora são ajustados para minimizar a entropia cruzada desta rede em função dos dados reais (classe \emph{verdadeira}) e das saídas da rede discriminadora (classe \emph{falsa}). Os pesos da rede geradora são ajustados para maximizar a entropia cruzada na saída da rede discriminadora, em função das saídas da rede discriminadora. Nesta etapa o objetivo é a rede geradora \emph{enganar} a rede discriminadora, para que classifique as saídas da geradora na classe \emph{verdadeira}. Como os objetidos são conflitantes, os autores propõe ajustar os pesos de cada rede de forma intercalada, ou seja, não são ajustados simultaneamente. Ademais, também comentam que existe um balanço delicado entre treinar \emph{demais} a rede discriminadora a ponto de deixar a tarefa da rede geradora \emph{muito difícil} e não conseguir convergência.

        Na proposta original a rede geradora recebe como entrada um vetor aleatório. O que se busca é que a rede geradora consiga mapear este vetor aleatório no espaço de distribuição dos dados reais. Uma rede geradora competente consegue aproximar o espaço de distribuição dos dados reais de forma a se tornar quase indistinguível do espaço real. Ao conseguir se tornar uma rede competente, a rede geradora estará efetivamente utilizando o vetor de entrada como uma \emph{codificação} das principais características dos dados reais.

\section{Questão 2}

    (1,2) Explique os conceitos de mapa de características (feature map), neurônio, campo receptivo e compartilhamento de pesos no contexto de uma camada convolucional de uma CNN (convolutional neural network).

    \subsection{Resposta}

        \begin{itemize}
            \item \textbf{Mapa de características}: Uma rede com camadas convolucionais funciona como uma série de filtros não-lineares, em cascata (no caso de mais de uma camada convolucional na rede). O que se entende como mapa de características são os resultados gerados por cada uma das camadas convolucionais. O ajuste dos pesos dos filtros objetivam encontrar características nos dados de entrada que sejam úteis para o problema posto.
            \item \textbf{Neurônio}: O neurônio é a unidade básica de compução de uma rede neural. Em uma camada densa cada neurônio se conecta a todos os dados de entrada. Em uma camada convolucional o neurônio de conecta a um número limitado de dados de entrada, e depois \emph{passeia} pelos dados (operação de convolução). Cada \emph{canal} na saída de uma camada convolucional é o resultado da aplicação de um mesmo neurônio seguidamente a porções dos dados de entrada.
            \item \textbf{Campo receptivo}: Uma camada densa trabalha \emph{observando} toda a informação de uma única vez, enquanto que uma camada convolucional \emph{observa} os dados de entrada em pequenas porções. Cada saída da camada convolucional é o resultado da aplicação do filtro (\emph{kernel}) em uma porção limitada dos dados. Ao ter camadas convolucionais seguidas o efeito é que cada saída das camadas mais profundas são o resultado da composição destas regiões em cascata. O campo receptivo de uma camada é o resultado desta composição de \emph{lentes} de volta para o dado de entrada, onde as saídas de camadas mais profundas são o resultado de regiões cada vez maiores dos dados de entrada.
            \item \textbf{Compartilhamento de pesos}: Uma camada convolucional pode ser entendida como uma camada densa em que os pesos são compartilhados entre os diferentes neurônios. As operações envolvidas são as mesmas de uma camada densa, mas é como se cada neurônio tivesse pesos ajustáveis (e não-nulos) apenas para as conexões a um número limitado de dados de entrada. Além disso os pesos utilizados são os mesmos entre estes diferentes neurônios. Este mecanismo ajuda a entender a grande diferença entre o número de pesos ajustáveis de camadas densas e camadas convolucionais.
        \end{itemize}


\section{Questão 3}

    Tendo em mente a teoria subjacente às máquinas de vetores-suporte (SVMs, do inglês support-vector machines):

    \begin{enumerate}[(a)]
        \item (0,5) Defina margem de classificação no contexto de um problema linearmente separável e explique por que a maximização da margem é, intuitivamente, uma abordagem segura de projeto.
        \item (0,5) Por que a formulação matemática relacionada à obtenção do classificador de máxima margem possui similaridades com as ideias de regularização (e.g. Tikhonov / ridge regression) vistas no curso?
    \end{enumerate}

    \subsection{Resposta \textbf{(a)}}

        Ao maximixar a margem de classificação de um problema linearmente separável estamos efetivamente maximizando a \emph{distância} do hiperplano classificador para os dados. Como não temos garantias dos dados de treino serem suficientes para definir exatamente a região correspondente a cada classe, novos dados (de teste) podem se posicionar na região \emph{entre} as classes. Maximizando a margem existe uma maior chance deste dados serem classificados corretamente (se posicionando no \emph{lado correto} do hiperplano que separa as classes).

    \subsection{Resposta \textbf{(b)}}

        A ideia da regularização é de reduzir a flexibilidade do modelo utilizado, fazendo uma troca entre viés e variância. O objetivo é que um modelo mais \emph{bem comportado} consiga generalizar melhor que um modelo mais flexível, e que possa sofrer com \emph{overfitting}.

        Entre duas classes linearmente separáveis é possível definir infinitos hiperplanos classificadores. A ideia do SVM é de reduzir a liberdade deste hisperplano, usando como critério a máxima margem, com o intuito de ter ter um classificador que generalize melhor. Um classificador de máxima margem se apoia nos dados mais \emph{difíceis} de serem classificados (mais próximos da outra classe), de forma que aumenta a chance de que novos dados que venham a se aproximar ainda mais da outra classe (mais \emph{difícieis} de classificar que os dados de treinados) sejam corretamente classificados.

\section{Questão 4}

    (1,2) Numa importante conferência da área de inteligência computacional, foi lançada uma competição no âmbito de um problema de classificação de imagens. Duas jovens pesquisadoras decidiram, então, empregar uma CNN já consolidada na literatura (e.g. uma ResNet) para resolver a tarefa. Partindo de uma inicialização aleatória para os pesos da rede escolhida, e utilizando apenas os dados disponíveis na competição, elas, ao final do treinamento, observaram um nível de acurácia bastante adequado. No entanto, quando exposta a novos padrões de entrada (conjunto de teste), esta rede não atingiu um bom desempenho, tendo ficado muito abaixo das expectativas iniciais de projeto. Recomende (com argumentos bem fundamentados) duas estratégias para amenizar este problema.

    \subsection{Resposta}

        Uma possibilidade é que a base de dados utilizada pelas pesquisadoras não tem tramanho adequado para o treinamento de uma rede muito profunda. Redes como a ResNet são muito profundas, com um número significativo de pesos a serem ajustados. Estas redes funcionam bem porque foram ajustadas com um número \textbf{muito} grande de dados. Uma forma de buscar contornar o problema do limitado número de dados é utilizar \emph{data augmentation}, gerandos amostras adicionais a partir dos próprio dados (rotação, translação, ...). É uma estratégia um pouco limitada, pois depende muito da quantidade de dados disponível, que pode ainda ser insuficiente. Pode ser mais viável associar o \emph{data augmentation} à utilização de apenas parte da rede profunda conhecida, de forma a ter uma melhor relação entre a quantidade de amostras (para treino, validação e teste) e o número de pesos ajustáveis.

        Uma estratégia mais interessante é fazer \emph{transfer learning}, ou seja, utilizar os pesos da rede já treinados e adaptar a estrutura da rede para o problema específico. Em diferentes publicações foi explorada a estrutura destas redes \emph{famosas}, verificando que as camadas vão se especializando em elementos cada vez mais complexo à medida que a informação segue pela rede. As primeiras camadas se especializam em formas simples (semi-círculo, linha vertical, ...), e camadas profundas em elementos mais complexos (olho, boca, ...). Boa parte destas atividades podem ser úteis para outros problemas de classificação. Assim é possível utilizar boa parte das primeiras camadas da rede existente, incluir algumas novas camadas e fazer o treinamento dos pesos apenas das novas camadas que foram adicionadas ao final. As últimas camadas da rede original também podem ser ajustados, mas partindo dos valores originalmente ajustados.

\section{Questão 5}

    (1,4) Considere o problema de identificação de tumores cerebrais a partir de um conjunto de 1500 imagens de ressonância magnética (MRI) do crânio, de dimensão 200 x 200, em tons de cinza (veja um exemplo na Figura 1).
    As possíveis classes do problema são: (0) glioma; (1) meningioma; (2) pituitário; (3) saudável (normal). Descreva detalhadamente como uma rede MLP (multilayer perceptron), com uma única camada intermediária, poderia ser aplicada a este problema, indicando aspectos referentes à arquitetura da rede, bem como à metodologia como um todo (e.g. tratamento dos dados, treinamento do modelo etc.).

    \subsection{Resposta}

        \noindent \textbf{Tratamento dos Dados}

        Os dados de entrada são imagens 200 x 200 em escala de cinza, logo podem ser representadas por uma matriz de 200 por 200 de números reais (ou inteiros). Uma operação opcional, mas que usualmente tem um efeito positivo é fazer a normalização dos dados de entrada. Uma opção é normalizar os valores entre 0 e 1.

        Uma etapa opcional é de incrementar a base de dados através do processo de \emph{data augmentation}. Novas amostras podem ser geradas a partir dos dados existentes. Variações das imagens originais podem ser gerada aplicando operações de rotação, translação, zoom, inversão (\emph{flip}), entre outros. As modificações apropriadas dependem das imagens da base de dados (eg.: girar em 180 graus uma imagem transversal de ressonância magnética pode não fazer sentido, pois todas tem a cabeça virada para a parte superior da imagem).

        Os dados das classes precisam ser codificados em um vetor \emph{one-hot} de tamanho 4, em que cada posição está associada a uma das classes. Por exemplo, o vetor associado a uma amostra classificada na classe glioma tem a forma $[1,0,0,0]$.

        Para utilizar estes dados de imagens em uma rede MLP temos primeiro que redimensionar cada matriz para um vetor (\emph{flatten}). Neste exemplo teremos vetores de $40\,000$ números reais.

        \noindent \textbf{Arquitetura da Rede}

        A camada de entrada da rede recebe os $40\,000$ valores de entrada de cada amostra. Cada neurônio da camada de entrada é associado a uma das posições do vetor de entrada, e recebe o respectivo valor da amostra. Logo, temos $40\,000$ neurônios. Não há pesos a serem ajustados nesta camada.

        A camada seguinte é a camada intermediária, onde teremos $n$ neurônios. Nesta camada densa cada neurônio estará conectado a todos os neurônios da camada anterior. Desta forma esta camada tem tem $40\,001n$ pesos a serem ajustados (para cada neurônio são $40\,000$ pesos que multiplicam os valores dos neurônios da camada de entrada e um peso associado ao \emph{bias}). Aos resultados destas somas ponderadas em cada neurônio é usualmente aplicada uma função não-linear (\emph{relu}, \emph{sigmóide}, \emph{tanh} etc.). Se não for aplicada uma função não-linear, a rede terá uma menor capacidade de resolver o problema proposto, pois só conseguirá encontrar relações lineares.

        A camada de saída é composta por 4 neurônios, um associado a cada classe. Cada neurônio se conecta a todos os $n$ neurônios da camada intermediária. Desta forma esta camada tem $3 (n+1)$ pesos ajustáveis. Ao resultado da soma ponderada é aplicada a função \emph{softmax}, que irá dar uma ideia de probabilidade associada a cada classe.

        \noindent \textbf{Treinamento do Modelo}

        Para o treinamento do modelo é uma boa prática separar os dados disponíveis entre treino, validação e teste. Os dados de treino serão efetivamente utilizados para controlar o processo de ajuste dos pesos da rede. Os dados de validação servem para acompanhar o treinamento da rede e evitar \emph{overfitting}, mas não são utilizados para guiar o algoritmo de ajuste. Os dados de teste são apresentados ao modelo apenas no final do processo de ajuste, servindo de indicador da qualidade da rede.

        Para o ajuste dos pesos são usualmente utilizados algoritmos baseados no gradiente da função de perda. Este algoritmos utilizam o gradiente para indicar a direção em que devem ser modificados os valores dos pesos de forma a minimizar a função de perda. A direção de minimização da função de perda é a direção contrária ao gradiente. O passo a ser dado pelo algoritmo de otimização é usualmente definido pelo usuário. Algoritmos de segunda ordem (com cálculo, ou aproximação, da Hessiana) conseguem uma estimativa da direção e do passo a ser dado, mas são menos comuns.

        O gradiente é calculado por retro-propagação (\emph{backpropagation}) das derivadas. Uma amostra passa pela rede, fazendos os cálculos até o final. A retro-propagação começa do final do cálculo e vai retornando até o início da rede, calculando as derivadas associadas a cada peso da rede.

        Para problemas de classificação a função de perde utilizada é normalmente a entropia cruzada. A minimização da entropia cruzada equivale a maximizar a verossimilhança (maximiza as probabilidades associadas às classes corretas de cada amostra dos dados de treino).

        A estrutura geral dos algoritmos de otimização utilizados é a mesma. É apresentado um conjunto de dados de treino (o número de amostras por \emph{batch} pode variar de um a todas as amostras) e a média dos gradientes associados a cada amostra define a direção de alteração dos pesos. O tamanho do passo é informado pelo usuário (diretamente ou indiretamente). Após todos os dados de treino terem sido apresentados à rede, considera-se que acabou esta \textbf{época}. Ao final da época é feita uma avaliação da rede com os dados de validação. O processo de ajuste dos pesos é repetido até que algum critério de parada é atingido (número máximo de épocas, valor mínimo da função de perda, valor mínimo da redução da função de perda entre épocas, não melhora da função de perda com os dados de validação após um determinado número de épocas etc.).

        Ao final do treino são aplicados os dados de teste à rede ajustada, e as métricas de interesse (\emph{f1-score}, acurácia balanceada etc.) são calculadas e avaliadas.


\section{Questão 6}

    Ao realizar um processo de PCA sobre uma base de dados, um pesquisador obteve os seguintes autovalores para a matriz de autocorrelação:

    \begin{equation}
        \mathbf{\lambda{}} =
        \begin{bmatrix}
            0,2 \\
            2,3 \\
            1,5 \\
            3 \\
            0,05 \\
            0,15 \\
            0,5 \\
            0,3
        \end{bmatrix}
    \end{equation}

    \begin{enumerate}[(a)]
        \item (0,3) Qual é a dimensão do espaço original dos dados?
        \item (0,7) Caso se busque uma preservação de ao menos 90\% do conteúdo energético dos dados, qual é o menor número possível de componentes principais empregadas? Justifique.
    \end{enumerate}

    \subsection{Resposta \textbf{(a)}}

        Assumindo uma matriz de autocorrelação de posto completo, e dado que a matriz de autocorrelação é aproximada pela média dos produtos externos dos dados de entrada, o número de autovalores coincide com a dimensão do espaço original: 8. O caso mais geral é que a dimensão do espaço original é ao menos 8.

    \subsection{Resposta \textbf{(b)}}

        A parcela de variância associada a cada componente principal é dada pela razão entre o valor do autovalor associado e a soma de todos os autovalores. Para preservar ao menos 90\% da variância observada nos dados (conteúdo energético) precisamos usar ao menos os 4 componentes principais associados aos 4 maiores autovalores.

        \begin{table}[H]
            \centering
            \begin{tabular}{c c c c}
                \toprule
                \textbf{$i$} & \textbf{$\lambda{}_i$} & \textbf{$\sum{}_{j=1}^i\lambda{}_j$} & \textbf{$\frac{\sum{}_{j=1}^i\lambda{}_j}{\sum{}_{j=1}^n\lambda{}_j}$} \\
                \midrule
                1 & 3.00 & 3.00 & 0.375 \\
                2 & 2.30 & 5.30 & 0.663 \\
                3 & 1.50 & 6.80 & 0.850 \\
                4 & 0.50 & 7.30 & 0.913 \\
                5 & 0.30 & 7.60 & 0.950 \\
                6 & 0.20 & 7.80 & 0.975 \\
                7 & 0.15 & 7.95 & 0.994 \\
                8 & 0.05 & 8.00 & 1.000 \\
                \bottomrule
            \end{tabular}
        \end{table}


\section{Questão 7}

    Considere que uma CNN tenha sido aplicada à classificação de imagens de retina em três classes: normal, glaucoma e catarata. Na etapa de teste, a seguinte matriz de confusão foi obtida:

    \begin{table}[H]
        \centering
        \begin{tabular}{l c c c}
            \toprule
             & \multicolumn{3}{c}{\textbf{Classe estimada}} \\
            \textbf{Classe real} & \textbf{Normal} & \textbf{Glaucoma} & \textbf{Catarata} \\
            \midrule
            Normal & 410 & 15 & 25 \\
            Glaucoma & 10 & 100 & 40 \\
            Catarata & 55 & 25 & 100 \\
            \bottomrule
        \end{tabular}
    \end{table}

    \begin{enumerate}[(a)]
        \item (0,4) Considerando a classe \textbf{Glaucoma} como positiva, obtenha as quantidades de verdadeiros positivos (TP), falsos positivos (FP), verdadeiros negativos (TN) e falsos negativos (FN).
        \item (0,6) Determine o valor da acurácia balanceada atingida pela CNN, mostrando explicitamente os valores das métricas intermediárias necessárias para o cálculo.
    \end{enumerate}

    \subsection{Resposta \textbf{(a)}}

        Para encontrar os valores considerando Glaucoma como classe positiva temos que agregar os valores das demais classes:

        \begin{enumerate}
            \item O número de verdadeiros positivos permanece o mesmo.
            \item A coluna de classe positiva estimada tem o mesmo total que a coluna de Glaucoma estimado da tabela anterior.
            \item A linha de classe positiva real tem o mesmo total que a linha de Glaucoma real da tabela anterior.
            \item O número de negativos verdadeiros é a soma dos termos fora da linha ou coluna Glaucoma da tabela anterior.
        \end{enumerate}

        \begin{table}[H]
            \centering
            \begin{tabular}{l c c}
                \toprule
                & \multicolumn{2}{c}{\textbf{Classe estimada}} \\
                \textbf{Classe real} & \textbf{Positiva} & \textbf{Negativa} \\
                \midrule
                Positiva & TP=100 & FN=50 \\
                Negativa & FP=40 & TN=590 \\
                \bottomrule
            \end{tabular}
        \end{table}

    \subsection{Resposta \textbf{(b)}}

        A acurácia balanceada é a média do \emph{recall} para cada classe. E \emph{recall} é a taxa de positivos verdadeiros: $TPR = \frac{TP}{TP+FN}$. O cálculo dos valores de TP e FN de cada classe seguem a lógica apresentada no item anterior. A acurácia balanceada da CNN é \textbf{0.711}.

        \begin{table}[H]
            \centering
            \begin{tabular}{l c c c}
                \toprule
                \textbf{Classe} & \textbf{TP} & \textbf{FN} & \textbf{\emph{Recall}} \\
                \midrule
                Normal   & 410 & 40 & 0.911 \\
                Glaucoma & 100 & 50 & 0.667 \\
                Catarata & 100 & 80 & 0.556 \\
                \midrule
                         & \multicolumn{2}{r}{\textbf{Média}} & 0.711 \\
                \bottomrule
            \end{tabular}
        \end{table}

\section{Questão 8}

    \begin{enumerate}[(a)]
        \item (1,0) Explique como funciona o mecanismo de auto-atenção de um transformer.
        \item (0,6) De que maneira um transformer consegue aproveitar possíveis interdependências de curto e longo prazo em uma sequência de entrada apesar de não ter recorrência?
    \end{enumerate}

    \subsection{Resposta \textbf{(a)}}
        O mecanismo de auto-atenção de um transformer ajusta a posição de cada elemento no espaço latente em função de todos os elementos da sequência. Os dados de entrada são os elementos em algum espaço latente (eg.: a localização de cada palavra em um espaço semântico). É montada uma matriz de auto-correlação calculando o produto interno de todos os possíveis pares de elementos (inclusive de cada elemento com ele mesmo). Estes produtos internos dão uma ideia do grau de correlação entre os elementos. Cada elemento é \emph{reescrito} como uma combinação linear convexa de todos os elementos ponderada pelos produtos internos calculados (os produtos internos passam por uma \emph{softmax} antes de multiplicar os elementos). Esta transformação acaba por mudar a posição de cada elemento em função do seu grau de correlação com os demais elementos da sequência(eg.: muda o sentido semântico de cada palavra em função das palavras que compõem a frase), criando uma ideia de contexto para os elementos da sequência.

        A este mecanismo podem ser adicionadas matrizes que \emph{ajustam} as posições de cada um dos elementos da sequência antes em cada operação. Uma matriz pode multiplicar os elementos que entram no primeiro termo de cada produto interno ($M_{query}$), outra o elemento que entra no segundo termo do produto interno ($M_{key}$) e uma terceira multiplica cada elemento que será utilizado na combinação convexa ($M_{value}$). Desta forma se dá mais flexibilidade à camada transformer para que busque representações alternativas da sequência (eg.: considerar uma frase como uma ironia ou uma afirmação).

    \subsection{Resposta \textbf{(b)}}

        O \emph{transformer} é um mecanismo do tipo vec2vec, de forma que recebe como entrada um vetor de elementos, e é perdida a informação de posição que uma sequência carrega. É preciso adicionar aos dados de entrada (já em algum espaço latente) um termo de codificação posicional. Cada elemento recebe um código único que representa a sua posição dentro da sequência original, que foi transformada em vetor. Na proposta original dos \emph{transformers} são utilizados os valores de uma família de senoides para codificar estas posições.

%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%

% \bibliographystyle{elsarticle-num}
% \bibliography{refs}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

%% \bibitem{label}
%% Text of bibliographic item

% \bibitem{}

% \end{thebibliography}


\end{document}
\endinput

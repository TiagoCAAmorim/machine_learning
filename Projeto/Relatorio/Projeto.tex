\documentclass[final,5p]{elsarticle}

% \documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}
\usepackage[portuguese]{babel}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{tablefootnote}

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepgfplotslibrary{statistics}
\usepackage{pgfplotstable}

\usepackage{placeins}
\usepackage{hyperref}
\numberwithin{equation}{section}

\usepackage{algorithm}
\usepackage[noEnd=true, indLines=true]{algpseudocodex}
\algrenewcommand\algorithmicrequire{\textbf{Entrada:}}
\algrenewcommand\algorithmicwhile{\textbf{Enquanto}}
\algrenewcommand\algorithmicrepeat{\textbf{Repete}}
\algrenewcommand\algorithmicuntil{\textbf{Até}}
\algrenewcommand\algorithmicif{\textbf{Se}}
\algrenewcommand\algorithmicthen{\textbf{então}}
\algrenewcommand\algorithmicelse{\textbf{Caso contrário}}
\algrenewcommand\algorithmicensure{\textbf{Objetivo:}}
\algrenewcommand\algorithmicreturn{\textbf{Retorna:}}
\algrenewcommand\algorithmicdo{\textbf{faça}}
\algrenewcommand\algorithmicforall{\textbf{Para todos}}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) \textcolor{black!50}{\emph{#1}}}

\newcommand*{\squareb}{\textcolor{black}{\rule{0.5em}{0.5em}}}
\newcommand*{\squareg}{\textcolor{gray}{\rule{0.5em}{0.5em}}}

\graphicspath{ {./png/} }

% \usepackage[fleqn]{nccmath}
% \usepackage{multicol}


%=========== Gloabal Tikz settings
% \pgfplotsset{compat=newest}
% \usetikzlibrary{math}
% \pgfplotsset{
%     height = 10cm,
%     width = 10cm,
%     tick pos = left,
%     legend style={at={(0.98,0.30)}, anchor=east},
%     legend cell align=left,
%     }
%  \pgfkeys{
%     /pgf/number format/.cd,
%     fixed,
%     precision = 1,
%     set thousands separator = {}
% }

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.98,0.98,0.98}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

% \journal{Nuclear Physics B}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Resolução de Sistema de Equações Lineares de Matrizes Pentadiagonais com Redes Neurais Convolucionais\tnoteref{label_title}}
\tnotetext[label_title]{Projeto final como parte dos requisitos da disciplina IA048: Aprendizado de Máquina.}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[label1]{Tiago C A Amorim (RA: 100675)}
\affiliation[label1]{organization={Doutorando no Departamento de Engenharia de Petróleo da Faculdade de Engenharia Mecânica, UNICAMP},
            city={Campinas},
            state={SP},
            country={Brasil}}

\author[label2]{Taylon L C Martins (RA: 177379)}
\affiliation[label2]{organization={Aluno especial, UNICAMP},
            city={Campinas},
            state={SP},
            country={Brasil}}


% \begin{abstract}

%     xxxxxxx

% \end{abstract}


%%Graphical abstract
% \begin{graphicalabstract}
%\includegraphics{grabs}
% \end{graphicalabstract}

%%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

\begin{keyword}
    Rede Neurais Convolucionais \sep Sistemas de Equações Lineares
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% main text
\section{Introdução}



\section{Motivação}

    O método das diferenças finitas é utilizado para resolver diferentes problemas físicos que podem ser descritos como equações diferenciais. O método se baseia na aproximação das derivadas por diferenças finitas (exemplos em \ref{eq:diffin}). O domínio espaço-temporal é discretizado e a solução das equações diferenciais é aproximada nos nós desta malha \cite{causon2010introductory}. O problema então é transformado em uma série de equações não-lineares, que usualmente são resolvidas por métodos numéricos, como o método de Newton-Raphson \cite{burden2016analise}.

    \begin{subequations}
        \begin{align}
            \frac{\partial u(x)}{\partial x} &\approx \frac{u(x+h) - u(x-h)}{2h} \\
            \frac{\partial^2 u(x)}{\partial x^2} &\approx \frac{u(x+h) - 2u(x) + u(x-h)}{h^2}
        \end{align}
        \label{eq:diffin}
    \end{subequations}

    É usual utilizar aproximações de derivada que utilizam os valores da função do nó e seus vizinhos diretos. Em problemas bidimensionais com malhas regulares (Figura \ref{fig:esquematico}) esta escolha leva a sistemas de equações com matrizes pentadiagonais (exemplo para uma malha $n_i$,$n_j$ em \ref{eq:sistema}). Cada equação não-linear tem termos associados a um dos nós (i,j) e seus quatro vizinhos: (i-1,j), (i+1,j), (i,j-1) e (i,j+1). A resolução numérica deste sistema de equações não-lineares usualmente está associada a métodos iterativos, em que novas equações lineares são resolvidas. Desta forma, a resolução do problema original está associada à solução de um significativo número de sistemas de equações lineares com matriz pentadiagonal.

    A proposta deste trabalho é avaliar a possibilidade de utilizar uma rede convolucional para resolver este tipo de sistema de equações lineares.

    \begin{figure}
    \begin{center}
        \caption{Esquemático de uma malha regular, com destaque para a célula (i,j) e seus vizinhos.}
        \label{fig:esquematico}
        \vspace{5pt}
    % \renewcommand{\arraystretch}{1.2}
        \begin{tabular}{c|c|c|c|c}
            & &  & & \\
            \hline
            & & (i,j-1) & & \\
            \hline
            & (i-1,j) & \textbf{(i,j)} & (i+1,j) & \\
            \hline
            & & (i,j+1) & & \\
            \hline
            & &  & & \\
        \end{tabular}
    \end{center}
    \end{figure}

    \begin{figure*}
        \begin{equation}
            \overbrace{
            \begin{bmatrix}
                a^0_1    & a^1_1    & 0       & \ldots  & 0      & a^{n_i}_1   & 0       &         & \ldots    &  0      \\
                a^{-1}_2   & a^0_2    & a^1_2 & 0       & \ldots & 0         & a^{n_i}_2 & 0       & \ldots    &  0      \\
                0          & a^{-1}_3   & a^0_3 & a^1_3 & 0      & \ldots    & 0       & a^{n_i}_3 & \ldots    & 0       \\
                \vdots     &            & \ddots  & \ddots  & \ddots &           &         &         & \ddots    & \vdots  \\
                0          & \ldots     & 0      & a^{-n_i}_{n_in_j-2}  & 0       & \ldots  & a^{-1}_{n_in_j-2}  & a^0_{n_in_j-2} & a^1_{n_in_j-2} & 0 \\
                0          & \ldots     &         & 0      & a^{-n_i}_{n_in_j-1}  & 0       & \ldots  & a^{-1}_{n_in_j-1}  & a^0_{n_in_j-1} & a^1_{n_in_j-1} \\
                0          & \ldots     &         &         & 0      & a^{-n_i}_{n_in_j}  & 0       & \ldots  & a^{-1}_{n_in_j}  & a^0_{n_in_j} \\
            \end{bmatrix}}^\textbf{A}
            \overbrace{
            \begin{bmatrix}
                x_1 \\
                x_2 \\
                x_3 \\
                \vdots \\
                x_{n_in_j-2} \\
                x_{n_in_j-1} \\
                x_{n_in_j} \\
            \end{bmatrix}}^\textbf{x}
            =
            \overbrace{
            \begin{bmatrix}
                b_1 \\
                b_2 \\
                b_3 \\
                \vdots \\
                b_{n_in_j-2} \\
                b_{n_in_j-1} \\
                b_{n_in_j} \\
            \end{bmatrix}}^\textbf{b}
            \label{eq:sistema}
        \end{equation}
    \end{figure*}


\section{Trabalhos Correlatos}

    Não foram encontrados muitos trabalhos com foco na resolução de sistemas de equações lineares com redes neurais. Duas formas distintas de resolução do problema foram propostas. A primeira vertente é resolver o sistema de equações lineares junto com o treinamento da rede (\cite{cichocki1992neural}). Uma aplicação interessante desta proposta é o de resolver sistema de grande dimensão, que possivelmente não cabem na memória disponível, e usar a rede para aprender um mapeamento que aproxima a resposta (\cite{gu2023deep}).

    Uma segunda vertente é a de treinar uma rede neural com base em vários exemplos de sistemas de equações a resolver. A rede treinada é utilizada para resolver novos sistemas de equações. Uma proposta focou na solução de sistemas tridiagonais (\cite{jiang2023neural}), utilizando uma série de camadas densas seguidas por conexões residuais. Um outro trabalho (\cite{kontolati2024learning}) foca na solução de problemas físicos associados a equações diferenciais. Este trabalho tenta primeiro encontrar uma representação densa dos dados de entrada por meio de uma rede \emph{autoenconder}. Posteriormente a representação densa de cada amostra passa por uma outra rede neural que busca resolver o problema.

    A proposta estudada neste projeto segue a segunda vertente. É feita a opção de utilizar camadas convolucionais para que a arquitetura da rede seja agnóstica à discretização do problema (tamanho da malha).

\section{Codificação do Sistema Linear}

    O objetivo da rede é resolver um problema do tipo $\textbf{Ax}=\textbf{b}$. Cada amostra da base de dados são os valores das diagonais da matriz $\textbf{A}$ e o vetor $\textbf{b}$, e a saída pretendida são os valores de $\textbf{x}$. Para simplificar a quantidade de dados a serem repassados à rede, é possível dividir as linhas da matriz $\textbf{A}$ e o vetor $\textbf{b}$ pelos valores da diagonal principal. Esta operação não muda o $\textbf{x}$ que resolve o sistema. Desta forma na nova matriz pentadiagonal todos os valores da diagonal principal são iguais à unidade.

    Os dados são organizados em forma de tensor 2D, à semelhança de uma imagem com ($n_i$,$n_j$) pixels. Seguindo a notação utilizada em \ref{eq:sistema}, os \emph{canais} desta imagem correspondem aos valores das diagonais $-n_i$, $-1$, $1$ e $n_i$, e os valores de $\textbf{b}$. Cada um destes vetores é ajustado para que os termos fiquem na posição (i,j) correspondente ao nó associado ao valor.

    A saída pretendida, $\textbf{x}$, também é formatada como um tensor 2D (com apenas um canal). Desta forma, a rede neural recebe uma \emph{imagem} ($n_i$,$n_j$) com 5 canais e deve gerar uma imagem de mesmo tamanho com 1 canal.

\section{Arquitetura e Treinamento da Rede Convolucional}

    Como as \emph{resolução} dos dados de entrada e saída de rede são as mesmas, optou-se por utilizar apenas camadas que mantém este tamanho. A rede é composta por (Figura \ref{fig:arquitetura}):

    \begin{enumerate}
        \item Camada convolucional com \emph{kernel}=1x1: passa de 5 para $n_{lat}$ o número de canais.
        \item $N$ camadas convolucionais com \emph{kernel}=3x3 e \emph{padding}=1.
        \item Camada convolucional com \emph{kernel}=1x1: passa de $n_{lat}$ para um canal.
    \end{enumerate}

    \begin{figure}[hbt!]
        \centering
        \includegraphics[width=0.95\columnwidth, trim={0cm 6cm 2cm 0cm}, clip]{./fig/NN.png}
        \caption{Arquitetura geral da rede neural proposta.}\label{fig:arquitetura}
    \end{figure}

    Existe

    A rede neural é treinada com amostras geradas aleatoriamente. A cada solicitação de uma nova amostra são gerados a matrix $\textbf{A}$ e o vetor $\textbf{x}$. O vetor $\textbf{b}$ é a multiplicação matricial dos dois primeiros termos. Em seguida os dados são reoganizados em forma de tensores.

    Para tornar a rede mais generalizável, diferentes distribuições probabilísticas são empregadas na geração de $\textbf{A}$ e $\textbf{x}$. Também é aplicada uma constante de escala aleatória à $\textbf{A}$, variando de $10^{-3}$ a $10^3$. Não é aplicada uma escala a $\textbf{x}$ para que a função de erro seja comparável entre as diferentes amostras.

    O treinamento da rede é feito com o algoritmo Adam. O passo de treinamento é reduzido à metade a cada 10 épocas sem redução no valor da função de perda dos dados de validação. A otimização é terminada se o valor da função de perda dos dados de validação não melhorar após 35 épocas.
    A função de perda é o RMSE (Equação \ref{eq:rmse}).

    \begin{equation}
        RMSE = \sqrt{\frac{1}{n_in_j}\sum_{i=1}^{n_i} \sum_{j=1}^{n_j} (y_{i,j} - \hat{y}(x_{i,j}))^2} \label{eq:rmse}
    \end{equation}



\section{Resultados}





    Todo o código foi desenvolvido em Pytorch, e está disponível em \href{https://github.com/TiagoCAAmorim/machine\_learning/blob/main/Projeto/solve_lin_eq.ipynb}{https://github.com/TiagoCAAmorim/machine \_learning/blob/main/Projeto/solve\_lin\_eq.ipynb}.


    \section{Conclusão}



% \appendix

%     \section{Redes Neurais Construídas}



%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%

\bibliographystyle{elsarticle-num}
\bibliography{refs}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

%% \bibitem{label}
%% Text of bibliographic item

% \bibitem{}

% \end{thebibliography}


\end{document}
\endinput
